J. Duris et. al. assume that the covariance matrix is fixed for buckets of parameters in the beam line the beam line
    Verify with Damon that this assumption does not hold in our case
    If we can (which I don't think we can), then we could theoretically directly copy their approach
    Caveats;
        1.) this would scale very poorly as our parameter space increases
        2.) we would have to store this somewhere
        3.) if / when something changes all the calculations would have to be done again

Worth noting it doesn't seem like we are doing anything wrt length scales
    and that improved someone's numbers by a lot
    https://github.com/fmfn/BayesianOptimization/issues/287

https://peterroelants.github.io/posts/gaussian-process-tutorial/#Predictions-from-posterior


use a different kernel function that takes into account the time the data samples were taken at,
could either be:
    K_new(x_i, x_j, hyperparams, t_i, t_j, falloff) = K_old(x_i, x_j, hperparams) * (falloff ** abs(t_i - t_j))
OR
    K_new(x_i, x_j, hyperparams, t_i, t_j) = K_old(x_i + [t_i], x_j + [t_j], hperparams)
    # NOTE second option would also include an additional hyperparam that is essentially equivalent to falloff
# NOTE would also have to monkeypatch the bayesian optimization library or I could write our own one


Other Notes:

also want to consider coding bayesian optimization using gaussian regression instead of this library because this library hides A LOT of tiny details that seem to be important (i.e. the length scales, some of the Matern kernel params)
    alternatively use another library?
    It would not take very long as the basic bayesian optimization process is quite simple
        1.) make model (would mostly be handled by sklearn)
        2.) probe n random points from kernel (trivial to implement)
        3.) get the point with the highest value (UCB, etc. [1]) for our chosen kernel (trivial to implement)
        4.) add that point to the model (trivial to implement with sklearn)
        5.) GOTO 2

loss function should probably ignore low values to a threshold as Harvey suggested at some point
    it's worth asking the operators what their thoughts on how this function should look

want to look into using the code harvey wrote and then grid/random/genetic/idk searching over possible kernels

[1] https://people.eecs.berkeley.edu/~kandasamy/talks/electrochem-bo-slides.pdf#page=36

